import pytest
import torch
import math
from src.model.token_decoder import TokenDecoder

class TestTokenDecoder:
    @pytest.fixture
    def decoder(self):
        """Returns a TokenDecoder instance with small dimensions for testing."""
        return TokenDecoder(
            vocab_size=100,
            embed_dim=32,
            num_heads=4,
            num_layers=2,
            dropout=0.1,
            max_seq_len=20
        )

    def test_initialization(self, decoder):
        """Test if the decoder initializes correctly."""
        assert isinstance(decoder, TokenDecoder)
        assert decoder.embed_dim == 32
        assert decoder.output_head.out_features == 100
        # Check if layers are created
        assert len(decoder.transformer_decoder.layers) == 2

    def test_generate_square_subsequent_mask(self, decoder):
        """Test causal mask generation."""
        sz = 5
        mask = decoder.generate_square_subsequent_mask(sz)

        # Check shape
        assert mask.shape == (sz, sz)

        # Check causal property: upper triangle should be -inf (masked), lower including diag should be 0.0
        # The mask generated by generate_square_subsequent_mask logic:
        # (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)
        # effectively creates a lower triangular matrix of ones.
        # Then it masks 0s with -inf and 1s with 0.0.

        # The diagonal and below should be 0.0 (visible)
        assert mask[0, 0] == 0.0
        assert mask[1, 0] == 0.0
        assert mask[1, 1] == 0.0

        # Above diagonal should be -inf (masked)
        assert mask[0, 1] == float('-inf')
        assert mask[0, 2] == float('-inf')

    def test_forward_pass_shape(self, decoder):
        """Test forward pass output shape."""
        batch_size = 2
        tgt_seq_len = 5
        src_seq_len = 10
        embed_dim = 32

        # Create dummy inputs
        tgt_tokens = torch.randint(0, 100, (batch_size, tgt_seq_len))
        memory = torch.randn(batch_size, src_seq_len, embed_dim)

        output = decoder(tgt_tokens, memory)

        # Expected shape: (Batch, TgtSeqLen, VocabSize)
        expected_shape = (batch_size, tgt_seq_len, 100)
        assert output.shape == expected_shape

    def test_forward_pass_with_mask(self, decoder):
        """Test forward pass with explicit mask."""
        batch_size = 2
        tgt_seq_len = 5
        src_seq_len = 10
        embed_dim = 32

        tgt_tokens = torch.randint(0, 100, (batch_size, tgt_seq_len))
        memory = torch.randn(batch_size, src_seq_len, embed_dim)

        # Create a custom mask
        mask = decoder.generate_square_subsequent_mask(tgt_seq_len)

        output = decoder(tgt_tokens, memory, tgt_mask=mask)

        assert output.shape == (batch_size, tgt_seq_len, 100)

    def test_forward_pass_dimension_mismatch(self, decoder):
        """Test that forward pass raises error on dimension mismatch."""
        batch_size = 2
        tgt_seq_len = 5
        src_seq_len = 10
        wrong_embed_dim = 16  # Should be 32

        tgt_tokens = torch.randint(0, 100, (batch_size, tgt_seq_len))
        # Memory with wrong embedding dimension
        memory = torch.randn(batch_size, src_seq_len, wrong_embed_dim)

        with pytest.raises(RuntimeError):
            decoder(tgt_tokens, memory)

    def test_pos_encoding_applied(self, decoder):
        """Test that positional encoding is applied (indirectly via dropout/value change)."""
        # Create a deterministic input (all zeros) for embedding
        # Embedding(0) will be a vector.
        # If we pass 0s, and pos encoding is added, the output before decoder layers
        # should vary by position if we could inspect it.
        # However, testing internals is fragile.
        # We can check that changing position changes output for same token.

        batch_size = 1
        tgt_seq_len = 2
        src_seq_len = 2
        embed_dim = 32

        # Sequence of identical tokens: [0, 0]
        tgt_tokens = torch.zeros((batch_size, tgt_seq_len), dtype=torch.long)
        memory = torch.zeros((batch_size, src_seq_len, embed_dim))

        # Run forward pass
        # Since we have dropout, we need to set eval mode to make it deterministic
        # BUT positional encoding dropout might still be active?
        # Wait, decoder.eval() disables dropout.
        decoder.eval()

        with torch.no_grad():
            output = decoder(tgt_tokens, memory)

        # Output at position 0 vs position 1 should be different despite same input token
        # because of positional encoding.
        out0 = output[0, 0, :]
        out1 = output[0, 1, :]

        assert not torch.allclose(out0, out1), "Outputs for same token at different positions should differ due to PE"
